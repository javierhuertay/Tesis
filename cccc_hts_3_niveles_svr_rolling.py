# -*- coding: utf-8 -*-
"""CCCC_HTS_3_niveles SVR_rolling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BSWMVCHWKmxIVbQ8gu49C6hLGPZM91qh
"""

pip install scikit-hts[all]

import pandas as pd
import numpy as np
import hts

import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.simplefilter("ignore")

# settings
plt.style.use('seaborn')
plt.rcParams["figure.figsize"] = (16, 8)

# Commented out IPython magic to ensure Python compatibility.
# %config InlineBackend.figure_format = 'retina'
# %xmode Context

"""## Data"""

from google.colab import drive
drive.mount("/content/drive", force_remount=True)
import os
import json

with open("/content/drive/MyDrive/Tesis/achs.json") as f:
  data = json.load(f)
# load the data 
df = pd.read_csv("/content/drive/MyDrive/Tesis/dotames.csv")
for display in data['routingPoints']['L']:
  df.loc[df.object_id == display['M']['originalName']['S'], 'object_id'] = display['M']['displayName']['S']

df_rp = pd.read_csv("/content/drive/MyDrive/Tesis/diccionario_rp.csv")
for index,row in df_rp.iterrows():
  try:
    df.loc[df.object_id == row['Cola'].strip(' '), ['supervisor','grupo']] = row['Supervisor'], row['Grupo']
  except:
    print(row['Cola'])

df.drop(columns='dow')
df = df.dropna()

df.isnull().values.any()

df['grupo'] = df['grupo'].replace(['Gestión ACHS'],'Gestión CCCC')

# lowercase the column names
df.columns = [col_name.lower() for col_name in df.columns]

# sum the calls over purpose
df = df.groupby(["hora","grupo","object_id"])["count_llamadas"] \
       .sum() \
       .reset_index(drop=False)

# create the bottom level id
df["grupo_object_id"] = df.apply(lambda x: f"{x['grupo']}_{x['object_id']}", axis=1)

df_total['total'].sum()

df.index = pd.to_datetime(df.hora)
df = df[(df.index.hour <= 21)]
df = df[(df.index.hour >= 10)]
#df = df[(df.index.month <= 1)]
#df = df[(df.index.weekday < 5)]
#df = df[(df.index.weekday )]

df.isnull().values.any()

index = range(len(df.grupo))
df.index = index

# inspect all the object_id per group
df.groupby("grupo")['object_id'].apply(set).to_frame()

# create the bottom level df
df_third_level = df.pivot(index="hora", columns="grupo_object_id", values="count_llamadas") \
                  .sort_values(by='hora',ascending=True,key=pd.to_datetime)
#df_third_level = df_third_level.fillna(0)           

# create the middle level df
df_second_level = df.groupby(["hora", "grupo"]) \
                    .sum() \
                    .reset_index(drop=False) \
                    .pivot(index="hora", columns="grupo", values="count_llamadas") \
                    .sort_values(by=['hora'],ascending=True,key=pd.to_datetime)

#df_second_level = df_second_level.fillna(0)           

# create the total level df
df_total = df.groupby("hora")["count_llamadas"] \
             .sum() \
             .to_frame() \
             .rename(columns={"count_llamadas": "total"}) \
             .sort_values(by=['hora'],ascending=True,key=pd.to_datetime)

# join the DataFrames
hierarchy_df = df_third_level.join(df_second_level) \
                              .join(df_total)
hierarchy_df.index = pd.DatetimeIndex(hierarchy_df.index)
hierarchy_df = hierarchy_df.resample("15T") \
                           .sum()
#hierarchy_df = hierarchy_df.asfreq('15T')
#hierarchy_df.drop(index)
print(f"Number of time series at the third level: {df_third_level.shape[1]}")
print(f"Number of time series at the second level: {df_second_level.shape[1]}")

df_total.index = pd.DatetimeIndex(df_total.index)
df_total = df_total.resample('15T').sum()

df_second_level.index = pd.DatetimeIndex(df_second_level.index)
df_second_level = df_second_level.resample('15T').sum()

df_third_level.index = pd.DatetimeIndex(df_third_level.index)
df_third_level = df_third_level.resample("15T") \
                           .sum()

hierarchy_df = hierarchy_df.resample('15T').sum()

promedios1 = {}
suma = 0
for column in df_second_level.columns:
  suma += df_second_level[column].sum()
for column in df_second_level.columns:
  promedios1[column] = df_second_level[column].sum()/suma

promedios2 = {}
for column in df_third_level.columns:
  promedios2[column] =  df_third_level[column].sum()/suma

"""## Creating the hierarchy"""

grupos = df["grupo"].unique()
colas = df['grupo_object_id'].unique()
total = {'total': list(grupos)}
grupo = {k: [v for v in colas if v.startswith(k)] for k in grupos}
hierarchy = {**total, **grupo}

from hts.hierarchy import HierarchyTree
ht = HierarchyTree.from_nodes(nodes=hierarchy, df=hierarchy_df)

"""## Visualizing the data"""

hierarchy_df["total"].plot(title="LLamadas - Nivel total")

ax = hierarchy_df[hierarchy['total']].plot(title="Llamadas - Nivel total")
ax.legend(bbox_to_anchor=(1.0, 1.0))

ax = hierarchy_df[hierarchy['DS 67']].plot(title="LLamadas - Mixi Armijo")
ax.legend(bbox_to_anchor=(1.0, 1.0))

df_total.sum()/(4*24*7*19)

df_second_level.describe()

sns.boxplot(x=df_total['total'])

fig, ax = plt.subplots(figsize=(16,8))
ax.scatter(df_total['total'], df_total.index)
ax.set_xlabel('Llamadas')
ax.set_ylabel('Fecha')
plt.show()

pip install empiricaldist

from empiricaldist import Pmf
for column in df_second_level.columns:
  pmf_poisson = Pmf.from_seq(df_second_level[column])
  pmf_poisson
  fig, ax = plt.subplots()
  ax.plot(pmf_poisson)
  ax.set(xlabel='Número de eventos', ylabel='P(X = x)',
        title=f'Función de probabilidad:{column}')
  ax.grid()
  plt.show()

df_second_level

pmf_poisson

df_second_level.median()

"""## Hierarchical time series forecasting"""

import matplotlib.dates as md
def plot_results(pred_df, source_df, cols_to_plot):
    """
    Helper function used for displaying the predictions vs. actuals for the indicated columns
    """
    
    if type(cols_to_plot) == str:
        cols_to_plot = list(cols_to_plot)
        
    for col in cols_to_plot:
        fig, ax = plt.subplots()
        print(list(pred_df.index.time))
        plt.plot(pred_df.index,pred_df[col],'bo', label='Predicho')
        plt.plot(pred_df.index,source_df[col],'go', label='Observado')
        #pred_df.plot(x= pred_df.index, y = pred_df[col], ax=ax,kind='scatter', label="Predicho")
        #source_df.plot(x= pred_df.index, y = pred.df[col], ax=ax,kind='scatter', label="Observedado")
    
        ax.legend()
        ax.set_title(col)
        ax.set_xlabel("Día")
        ax.set_ylabel("Llamdas")
        ax.xaxis.set_major_locator(md.MinuteLocator(interval = 15))
        xlocator = md.MinuteLocator(byminute=[0,15,30,45], interval = 1)
        ax.xaxis.set_major_locator(xlocator)
        fig.autofmt_xdate()

"""### Predictions with Rolling

"""

from sklearn import svm
from sklearn.datasets import make_regression
from sklearn.multioutput import MultiOutputRegressor
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from numpy import sqrt
from sklearn.preprocessing import StandardScaler

def rolling(df, quantity, steps): #Steps in the future
  # prepare data
  X = df.values
  size = int(len(X)*.7)
  #xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.15)
  X_train, X_test = X[0:size], X[size:len(X)]
  predictions = []
  y = []
  rmse = []
  # Rolling Forecasting Origin
  for predict in range(0,quantity,1): # From 0 to quantity-1.
    size_rolling = len(X_train) - predict - 1 # At what position we are, keeps shrinking
    begining = quantity - predict # Where we start, keeps shrinking
    X_rolling = X_train[begining:size_rolling]
    y_rolling = X_train[X_rolling.shape[0]:] # Starts from where X ends
    svr_rbf = (SVR(kernel='rbf',C=1e3,gamma=0.4))
    svr_rbf.fit(X_rolling.T,y_rolling[predict].T)
    output = svr_rbf.predict(X_rolling.T)
    predictions.append(output)
    y.append(y_rolling[predict].T)
    # evaluate forecasts
    #rmse = sqrt(mean_squared_error(y_rolling[predict].T, output)
    rmse.append(sqrt(mean_squared_error(y_rolling[predict].T, output)))
    print("RMSE: {}".format(np.mean(rmse)))
  # plot forecasts against actual outcomes
  plt.show()
  
  return predictions, y, X_test

predictions, y, X_test = rolling(df_third_level, 4, 1)

def steps(df, steps): #Steps in the future
  # prepare data
  X = df.values
  #size = int(len(X)*.7)
  #xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.15)
  X_train, X_test = X[0:len(X)-steps], X[-steps:]
  #print(X_train, X_test)
  predictions = []
  y = []
  rmse = []
  # Rolling Forecasting Origin
  for step in range(0,steps,1): # From 0 to quantity - 1.
    X_rolling = X_train[step:]
    y_rolling = X_test
    svr_rbf = (SVR(kernel='rbf',C=1e3,gamma=0.4))
    svr_rbf.fit(X_rolling.T,X_rolling[step])
    output = svr_rbf.predict(X_rolling.T)
    X_train = np.vstack([X_train, output])
    predictions.append(output)
    #y.append(y_rolling[predict].T)
    # evaluate forecasts
    #rmse = sqrt(mean_squared_error(y_rolling[predict].T, output)
    rmse.append(sqrt(mean_squared_error(X_test[step].T, output)))
    print("RMSE: {}".format(np.mean(rmse)))
  #plot forecasts against actual outcomes
  #plt.show()
  
  return predictions, X_test, rmse, steps

predictions_2, observations_2, rmse_2, step = steps(df_third_level,4)

predictions_1, observations_1, rmse_1, step = steps(df_second_level,4)

predictions_t, observations_t , rmse_t, step = steps(df_total, 4)

"""### Creating DF for different approaches.




"""

def mape(actual, pred): 
    actual, pred = np.array(actual), np.array(pred)
    return np.mean(np.abs((actual - pred) / actual))

df_predictions_2 = df_third_level[-step:]
df_predictions_1 = df_second_level[-step:]
df_predictions_t = df_total[:-step]
df_bu = df_predictions_2[0:0]
df_mo = df_predictions_1[0:0]
df_td = df_predictions_t[0:0]

i = 0
for hora in df_predictions_2.index:
  df_bu.loc[hora] = predictions_2[i]
  df_mo.loc[hora] = predictions_1[i]
  df_td.loc[hora] = predictions_t[i]
  i += 1

"""### Bottom-up SVR"""

df_1 = df_second_level[0:0]
for hora in df_bu.index:
    df_1.loc[hora] = 0

for col in df_bu.columns:
    for cols in df_1:
        if col.startswith(cols):
          df_1[cols] += df_bu[col]

df_0 = df_total[0:0]

for hora in df_1.index:
    df_0.loc[hora] = 0

df_0['total'] = df_1.sum(axis=1)

dfs_predicted = [df_bu, df_1, df_0]
dfs_observed = [df_third_level[-step:],df_second_level[-step:],df_total[-step:]]

rmse_bu = []
mape_bu = []
for df in range(len(dfs_predicted)):
  mape_bu.append(mape(dfs_predicted[df],dfs_observed[df]))
  rmse_bu.append(sqrt(mean_squared_error(dfs_predicted[df],dfs_observed[df])))
#print(mape_bu)
print(rmse_bu)

"""### Top-down SVR"""

df_1 = df_second_level[0:0]
df_2 = df_third_level[0:0]
for hora in df_td.index:
    df_1.loc[hora] = 0
    df_2.loc[hora] = 0

for col in df_1.columns:
  for key in promedios1.keys():
    if col == key:
      df_1[col] = promedios1[key]*df_td.values

for col in df_2.columns:
  for key in promedios2.keys():
    if col == key:
      df_2[col] = promedios2[key]*df_td.values

dfs_predicted = [df_2, df_1, df_td]
dfs_observed = [df_third_level[-step:],df_second_level[-step:],df_total[-step:]]

rmse_td = []
mape_td = []
for df in range(len(dfs_predicted)):
  mape_td.append(mape(dfs_predicted[df],dfs_observed[df]))
  rmse_td.append(sqrt(mean_squared_error(dfs_predicted[df],dfs_observed[df])))
#print(mape_td)
print(rmse_td)

df_total.index = pd.to_datetime(df_total.index)

df_t_bu = df_0

plot_results(df_td, df_total[-step:], cols_to_plot=['total'])

plot_results(df_t_bu, df_total[-step:], cols_to_plot=['total'])

"""### Middle-out SVR"""

df_0 = df_total[0:0]
df_2 = df_third_level[0:0]
for hora in df_td.index:
    df_0.loc[hora] = 0
    df_2.loc[hora] = 0

df_0['total'] = df_mo.sum(axis=1)

i = 0
for col in df_mo.columns:
  for cols in df_2.columns:
      if cols.startswith(col):
        df_2[cols] = list(promedios2.values())[i]*df_mo[col].values
        i += 1

dfs_predicted = [df_2, df_mo, df_0]
dfs_observed = [df_third_level[-step:],df_second_level[-step:],df_total[-step:]]

rmse_mo = []
mape_mo = []
for df in range(len(dfs_predicted)):
  mape_mo.append(mape(dfs_predicted[df],dfs_observed[df]))
  rmse_mo.append(sqrt(mean_squared_error(dfs_predicted[df],dfs_observed[df])))
#print(mape_mo)
print(rmse_mo)

plot_results(df_0, df_total[-step:], cols_to_plot=['total'])

last_four = hierarchy_df[-4:]

hierarchy_df = hierarchy_df[:-step]

"""### The bottom-up approach"""

model_bu_arima = hts.HTSRegressor(model='auto_arima', revision_method='BU', n_jobs=0)
model_bu_arima = model_bu_arima.fit(hierarchy_df, hierarchy)

pred_bu_arima = model_bu_arima.predict(steps_ahead=4)
rmse_bu_arima = list()

cols = pred_bu_arima.columns.tolist()
last_four = last_four[cols]

rmse_bu_arima = sqrt(mean_squared_error(last_four['total'], pred_bu_arima['total'][-step:])), sqrt(mean_squared_error(last_four.iloc[:,1:15], pred_bu_arima.iloc[:,1:15][-step:])), sqrt(mean_squared_error(last_four.iloc[:,16:], pred_bu_arima.iloc[:,16:][-step:]))

plot_results(pred_bu_arima[-step:], last_four, cols_to_plot=['total'])

model_bu_holt_winters = hts.HTSRegressor(model='holt_winters', revision_method='BU', n_jobs=0)
model_bu_holt_winters = model_bu_holt_winters.fit(hierarchy_df, hierarchy)
pred_bu_holt_winters = model_bu_holt_winters.predict(steps_ahead=4)

rmse_bu_hw = list()
rmse_bu_hw = sqrt(mean_squared_error(last_four['total'], pred_bu_holt_winters['total'][-step:])), sqrt(mean_squared_error(last_four.iloc[:,1:15], pred_bu_holt_winters.iloc[:,1:15][-step:])), sqrt(mean_squared_error(last_four.iloc[:,16:], pred_bu_holt_winters.iloc[:,16:][-step:]))

plot_results(pred_bu_holt_winters[-step:], last_four, cols_to_plot=['total'])

"""### The top-down approach"""

model_td_arima = hts.HTSRegressor(model='auto_arima', revision_method='PHA', n_jobs=0)
model_td_arima = model_td_arima.fit(hierarchy_df, hierarchy)
pred_td_arima = model_td_arima.predict(steps_ahead=4)

rmse_td_arima = list()
rmse_td_arima = sqrt(mean_squared_error(last_four['total'], pred_td_arima['total'][-step:])), sqrt(mean_squared_error(last_four.iloc[:,1:15], pred_td_arima.iloc[:,1:15][-step:])), sqrt(mean_squared_error(last_four.iloc[:,16:], pred_td_arima.iloc[:,16:][-step:]))

plot_results(pred_td_arima[-step:], last_four, cols_to_plot=['total'])

model_td_holt_winters = hts.HTSRegressor(model='holt_winters', revision_method='PHA', n_jobs=0)
model_td_holt_winters = model_td_holt_winters.fit(hierarchy_df, hierarchy)
pred_td_holt_winters = model_td_holt_winters.predict(steps_ahead=4)
rmse_td_hw = list()
rmse_td_hw = sqrt(mean_squared_error(last_four['total'], pred_td_holt_winters['total'][-step:])), sqrt(mean_squared_error(last_four.iloc[:,1:15], pred_td_holt_winters.iloc[:,1:15][-step:])), sqrt(mean_squared_error(last_four.iloc[:,16:], pred_td_holt_winters.iloc[:,16:][-step:]))

plot_results(pred_td_holt_winters[-step:], last_four, cols_to_plot=['total'])

"""### The middle-out approach




"""

model_mo_arima = hts.HTSRegressor(model='auto_arima', revision_method='FP', n_jobs=0)
model_mo_arima = model_mo_arima.fit(hierarchy_df, hierarchy)
pred_mo_arima = model_mo_arima.predict(steps_ahead=4)
rmse_mo_arima = list()
rmse_mo_arima = sqrt(mean_squared_error(last_four['total'], pred_mo_arima['total'][-step:])), sqrt(mean_squared_error(last_four.iloc[:,1:15], pred_mo_arima.iloc[:,1:15][-step:])), sqrt(mean_squared_error(last_four.iloc[:,16:], pred_mo_arima.iloc[:,16:][-step:]))

plot_results(pred_mo_arima[-step:], last_four, cols_to_plot=['total'])

model_mo_holt_winters = hts.HTSRegressor(model='holt_winters', revision_method='FP', n_jobs=0)
model_mo_holt_winters = model_mo_holt_winters.fit(hierarchy_df, hierarchy)
pred_mo_holt_winters = model_mo_holt_winters.predict(steps_ahead=4)
rmse_mo_hw = list()
rmse_mo_hw = sqrt(mean_squared_error(last_four['total'], pred_mo_holt_winters['total'][-step:])), sqrt(mean_squared_error(last_four.iloc[:,1:15], pred_mo_holt_winters.iloc[:,1:15][-step:])), sqrt(mean_squared_error(last_four.iloc[:,16:], pred_mo_holt_winters.iloc[:,16:][-step:]))

plot_results(pred_mo_holt_winters[-step:], last_four, cols_to_plot=['total'])

rmse_bu[0],rmse_bu[-1] = rmse_bu[-1], rmse_bu[0]
rmse_td[0],rmse_td[-1] = rmse_td[-1], rmse_td[0]
rmse_mo[0],rmse_mo[-1] = rmse_mo[-1], rmse_mo[0]

rmse_total = [rmse_bu, rmse_td, rmse_mo, rmse_bu_arima, rmse_td_arima, rmse_mo_arima, rmse_bu_hw, rmse_td_hw, rmse_mo_hw]
rmse_total

rmse_graph = list()
for i in rmse_total:
  rmse_graph.append(list(i))
